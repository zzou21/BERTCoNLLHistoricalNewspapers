{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d44d24f",
   "metadata": {},
   "source": [
    "This Notebook is used to qualitatively and quantitatively evaluate the NER labeling done by the fine-tuned BERT. This evaluation is done against the baseline metrics (seen in `baselineBERT.ipynb`) and the labels done by an NER model that has been giving high praises in the field (Stanford NER).\n",
    "\n",
    "I'm comparing how closely my fine-tuned model performs against the Stanford NER and whether both perform better than the baseline. At the end of the notebook, I note a major asterisk of my evaluation.\n",
    "- Since I do not have a gold standard NER labes (my data is new and has not undergone human annotation), I'm using Stanford NER's labeling as a frame of reference, as Stanford NER is a renowned NER tool. I assume that Stanford NER's labels are correct (like a \"silver standard\" rather than a \"gold standard\") and see how my model perform against Stanford NER's performance on the data.\n",
    "\n",
    "I have noted each of the comparison and evaluation that I'm running, and I have a final conclusion in the end.\n",
    "\n",
    "Code attribution:\n",
    "All code in this Notebook is mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stanford NER:\n",
    "\n",
    "import json\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "def stanfordNERTag(jsonContentPath, stanfordTagger):\n",
    "    with open(jsonContentPath, \"r\") as jsonFile:\n",
    "        jsonContentUntagged = json.load(jsonFile)\n",
    "    \n",
    "    taggingResultDict = {}\n",
    "\n",
    "    progressCounter = 0\n",
    "    totalNeedToProcess = len(jsonContentUntagged)\n",
    "    for newspaperName, newspaperContent in jsonContentUntagged.items():\n",
    "        taggingResultDict[newspaperName] = stanfordTagger.tag(newspaperContent.split())\n",
    "        progressCounter += 1\n",
    "        print(f\"Processed {progressCounter}/{totalNeedToProcess} files\")\n",
    "\n",
    "    with open(\"/Users/Jerry/Desktop/CS372/FinalProject/data/stanfordTaggingMerged(UseInEvaluation).json\", \"w\") as jsonTagResultFile:\n",
    "        json.dump(taggingResultDict, jsonTagResultFile, indent=4)\n",
    "    \n",
    "jsonContentPath = \"/Users/Jerry/Desktop/CS372/FinalProject/data/newspaperCleanedContent.json\"\n",
    "stanfordTagger = StanfordNERTagger('/Users/Jerry/Desktop/CS372/stanford-ner-2020-11-17/classifiers/english.muc.7class.distsim.crf.ser.gz', '/Users/Jerry/Desktop/CS372/stanford-ner-2020-11-17/stanford-ner-4.2.0.jar')\n",
    "\n",
    "taggingResults = stanfordNERTag(jsonContentPath, stanfordTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784d7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first compare the number of labels in both mine and Stanford's against the baseline (7%)\n",
    "\n",
    "# This function is a reusable function that finds the number of words in our corpus\n",
    "def findNumberOfWords(dataStorageJSON):\n",
    "    with open(dataStorageJSON, \"r\") as contentJSON:\n",
    "        rawContent = json.load(contentJSON)\n",
    "    \n",
    "    wordCount = {}\n",
    "    for newspaperTitle, newspaperContent in rawContent.items():\n",
    "        wordCount[newspaperTitle] = len(newspaperContent.split())\n",
    "    \n",
    "    totalNumberOfWords = sum([newspaperWordCount for newspaperTitle, newspaperWordCount in wordCount.items()])\n",
    "\n",
    "    return wordCount, totalNumberOfWords\n",
    "\n",
    "# This is a reusable function that finds the number of labels performed by a model\n",
    "def countNumberOfLabels(NERStorageJSON):\n",
    "    with open(NERStorageJSON, \"r\") as storageJSON:\n",
    "        NERContent = json.load(storageJSON)\n",
    "    \n",
    "    labelCounter = {}\n",
    "    for newspaperTitle, newspaperContent in NERContent.items():\n",
    "        labelCounter[newspaperTitle] = len(newspaperContent)\n",
    "    \n",
    "    totalNumberOfLabels = sum([newspaperLabelCount for newspaperTitle, newspaperLabelCount in labelCounter.items()])\n",
    "\n",
    "    return labelCounter, totalNumberOfLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ccfb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the two functions above\n",
    "\n",
    "# loads the raw data\n",
    "rawDataJSON = \"/Users/Jerry/Desktop/CS372/FinalProject/data/newspaperCleanedContent.json\"\n",
    "\n",
    "# Loads the labels done by Stanford NER (to compare against)\n",
    "stanfordNERJSON = \"/Users/Jerry/Desktop/CS372/FinalProject/data/stanfordTaggingMerged(UseInEvaluation).json\"\n",
    "\n",
    "# Load the labels done by my model\n",
    "projectNERJSON = \"/Users/Jerry/Desktop/CS372/FinalProject/notebooks/entitiesOutput.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8edd1dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in corpus: 1546660\n",
      "Stanford NER found: 51951 labels in total\n",
      "My model found: 8347 labels in total\n"
     ]
    }
   ],
   "source": [
    "wordCountByDoc, totalWordCount = findNumberOfWords(rawDataJSON)\n",
    "\n",
    "print(f\"Total number of words in corpus: {totalWordCount}\")\n",
    "\n",
    "stanfordLabelCountDoc, stanfordTotalLabelCount = countNumberOfLabels(stanfordNERJSON)\n",
    "projectLabelCountDoc, projectTotalLabelCount = countNumberOfLabels(projectNERJSON)\n",
    "\n",
    "print(f\"Stanford NER found: {stanfordTotalLabelCount} labels in total\")\n",
    "print(f\"My model found: {projectTotalLabelCount} labels in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34aa038",
   "metadata": {},
   "source": [
    "### Evaluation 1: number of labels\n",
    "\n",
    "This shows whether my model has found as many labels as the Stanford NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1001dbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NER has labeled 3.358915340152328% of words as Named Entites\n",
      "My model has labeled 0.5396790503407343% of words as Named Entities \n"
     ]
    }
   ],
   "source": [
    "# Now determine whether I have achieved more than 7%\n",
    "\n",
    "stanfordPercentage = (stanfordTotalLabelCount / totalWordCount) * 100\n",
    "projectPercentage = (projectTotalLabelCount / totalWordCount) *100\n",
    "\n",
    "print(f\"Stanford NER has labeled {stanfordPercentage}% of words as Named Entites\")\n",
    "print(f\"My model has labeled {projectPercentage}% of words as Named Entities \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d2719",
   "metadata": {},
   "source": [
    "Both mine and Stanford NER has labeled less words than the baseline. This does not necessarily mean that the Stanford NER is bad because we are only considering three types of Named Entites (Person, Location, and Organization) while the baseline considers all types of Named Entities.\n",
    "\n",
    "However, the fact that my own model has labeled much less Named Entites than Stanford's shows that my model is not performing as well as Stanford's.\n",
    "\n",
    "This shows that my model has identified less Named Entities. This is an example of having low `recall` score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61a471",
   "metadata": {},
   "source": [
    "### Evaluation 2: consistency of labels\n",
    "\n",
    "This shows whether my model is consistent in marking a word with a specific way regardless of how many times the word appears. This evaluation is built on the premise that Named Entites have specific meanings that do not change much regardless of usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0d60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at consistency in labeling Stanford:\n",
    "\n",
    "# This function turns a labeled JSON into a dictionary structured: {\"NERLabel\": [\"NE1\", \"NE2\"]...}\n",
    "def turnIntoLabelsContentDict(jsonFile):\n",
    "    with open(jsonFile, \"r\") as NERFile:\n",
    "        nerContent = json.load(NERFile)\n",
    "    \n",
    "    labelDict = {}\n",
    "    for newspaperTitle, newspaperContent in nerContent.items():\n",
    "        for text, label in newspaperContent:\n",
    "            if label in [\"ORGANIZATION\", \"LOCATION\", \"PERSON\"]:\n",
    "                if label not in labelDict:\n",
    "                    labelDict[label] = []\n",
    "                labelDict[label].append(text)\n",
    "    return labelDict\n",
    "\n",
    "stanfordLabelDict = turnIntoLabelsContentDict(stanfordNERJSON)\n",
    "projectLabelDict = turnIntoLabelsContentDict(projectNERJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9050ef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels used: dict_keys(['LOCATION', 'ORGANIZATION', 'PERSON'])\n",
      "Total number of words in dataset: 1546660\n",
      "Stanford all three overlap: 22\n",
      "Stanford Location and organization overlap: 129\n",
      "Stanford Location and Person overlap: 126\n",
      "Stanford person and organization overlap: 149\n"
     ]
    }
   ],
   "source": [
    "# Consistency metrics for Stanford NER:\n",
    "print(f\"Labels used: {stanfordLabelDict.keys()}\")\n",
    "print(f\"Total number of words in dataset: {totalWordCount}\")\n",
    "allThreeOverlapStanford = set(stanfordLabelDict[\"LOCATION\"]) & set(stanfordLabelDict[\"ORGANIZATION\"]) & set(stanfordLabelDict[\"PERSON\"])\n",
    "\n",
    "print(f\"Stanford all three overlap: {len(allThreeOverlapStanford)}\")\n",
    "print(f\"Stanford Location and organization overlap: {len(set(stanfordLabelDict['LOCATION']) & set(stanfordLabelDict['ORGANIZATION']))}\")\n",
    "print(f\"Stanford Location and Person overlap: {len(set(stanfordLabelDict['LOCATION']) & set(stanfordLabelDict['PERSON']))}\")\n",
    "print(f\"Stanford person and organization overlap: {len(set(stanfordLabelDict['PERSON']) & set(stanfordLabelDict['ORGANIZATION']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29740d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels used: dict_keys(['LOCATION', 'PERSON', 'ORGANIZATION'])\n",
      "Total number of words in dataset: 1546660\n",
      "My project's all three overlap: 34\n",
      "My project's Location and organization overlap: 82\n",
      "My project's Location and Person overlap: 138\n",
      "My project's person and organization overlap: 42\n"
     ]
    }
   ],
   "source": [
    "# Consistency metrics for my model:\n",
    "print(f\"Labels used: {projectLabelDict.keys()}\")\n",
    "print(f\"Total number of words in dataset: {totalWordCount}\")\n",
    "allThreeOverlapMyProject = set(projectLabelDict[\"LOCATION\"]) & set(projectLabelDict[\"ORGANIZATION\"]) & set(projectLabelDict[\"PERSON\"])\n",
    "\n",
    "print(f\"My project's all three overlap: {len(allThreeOverlapMyProject)}\")\n",
    "print(f\"My project's Location and organization overlap: {len(set(projectLabelDict['LOCATION']) & set(projectLabelDict['ORGANIZATION']))}\")\n",
    "print(f\"My project's Location and Person overlap: {len(set(projectLabelDict['LOCATION']) & set(projectLabelDict['PERSON']))}\")\n",
    "print(f\"My project's person and organization overlap: {len(set(projectLabelDict['PERSON']) & set(projectLabelDict['ORGANIZATION']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f0dd8",
   "metadata": {},
   "source": [
    "Stanfod NER has 22 words that are labeled in all three NER labels while mine has 34. This is a rather small difference compared to the large data size of 1546660 words.\n",
    "\n",
    "if a word appears in more than one NER label sets, we could generaly say that there is an inconsistency in NER labeling because the nature of a Named Entity -- a word or phrase that has specific meanings -- will not be used differently in different context. Although this is not absolute (as in there will be cases where a Named Entity could be used in different ways), it is still a reliable way of measuring the consistency.\n",
    "\n",
    "When it comes to overlaps between Location and Person specifically, my model is slightly less consistent than Stanford NER, where my model labeled 138 words twice while Stanford NER had only 126.\n",
    "\n",
    "However, for overlaps between Lcoation-Organization and Person-Organization, my model had less overlapping labels for both (82 and 42 to Stanford's 129 and 149, respectively).\n",
    "\n",
    "This means that my model is very comparable with Stanford NER in terms of consistency in labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53795228",
   "metadata": {},
   "source": [
    "### Evaluation 3: Qualitative analysis\n",
    "I will now visually determine whether the NER labels by my model is effective or not. To do so, I will randomly select 3 labels from 100 newspapers that have been labeled and read them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "219cb7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Montezuma Avenue', 'LOCATION'], ['11th St', 'LOCATION'], ['Ph', 'ORGANIZATION']]\n",
      "[['Miller', 'PERSON'], ['Pacific', 'LOCATION'], ['White mountains', 'LOCATION']]\n",
      "[['Mi', 'LOCATION'], ['rizona', 'LOCATION'], ['X', 'LOCATION']]\n",
      "[['H', 'PERSON'], ['Robinson', 'LOCATION'], ['Lawrence S. Hough', 'PERSON']]\n",
      "[['I', 'LOCATION'], ['I', 'PERSON'], ['E', 'LOCATION']]\n",
      "[['Standard Oil Company', 'ORGANIZATION'], ['Arizona', 'LOCATION'], ['Ariz', 'LOCATION']]\n",
      "[['Fleming', 'LOCATION'], ['t River Valley', 'LOCATION'], ['D', 'LOCATION']]\n",
      "[['.', 'PERSON'], ['G', 'LOCATION'], ['0', 'PERSON']]\n",
      "[['I', 'LOCATION'], ['W', 'LOCATION'], ['East Adams', 'LOCATION']]\n",
      "[['A', 'ORGANIZATION'], ['Phoenix', 'LOCATION'], ['rizona Daily Citizen', 'ORGANIZATION']]\n",
      "[['Arizona', 'LOCATION'], ['Moore', 'LOCATION'], ['Davidson', 'LOCATION']]\n",
      "[['Arizona', 'LOCATION'], ['Gil', 'LOCATION'], ['Arizona', 'LOCATION']]\n",
      "[['IZONA R', 'LOCATION'], ['AR', 'LOCATION'], ['Arizona', 'LOCATION']]\n",
      "[['Calif', 'LOCATION'], ['Black', 'LOCATION'], ['D', 'LOCATION']]\n",
      "[['W', 'LOCATION'], ['IZONA', 'LOCATION'], ['ickenburg', 'LOCATION']]\n",
      "[['Calif', 'LOCATION'], ['New Mexico', 'LOCATION'], ['Kansas City', 'LOCATION']]\n",
      "[['L', 'PERSON'], ['W', 'LOCATION'], ['C', 'PERSON']]\n",
      "[['Arizona', 'LOCATION'], ['Phoenix', 'LOCATION'], ['Center', 'LOCATION']]\n",
      "[['South Central', 'LOCATION'], ['Co', 'LOCATION'], ['Pratt-Gilbert', 'ORGANIZATION']]\n",
      "[['Santa Rita', 'LOCATION'], ['Burrage', 'PERSON'], ['Burrages', 'PERSON']]\n",
      "[['Marguerite', 'PERSON'], ['Elizabeth Toohey', 'PERSON'], ['Eddie Rickenbacker', 'PERSON']]\n",
      "[['AR', 'LOCATION'], ['E', 'PERSON'], ['Heyman Fu', 'ORGANIZATION']]\n",
      "[['L', 'LOCATION'], ['Angeles', 'LOCATION'], ['E', 'LOCATION']]\n",
      "[['Goodye', 'ORGANIZATION'], ['AR', 'LOCATION'], ['yd', 'PERSON']]\n",
      "[['Market Street', 'LOCATION'], ['Han Francisco', 'LOCATION'], ['California', 'LOCATION']]\n",
      "[['H', 'LOCATION'], ['IZONA', 'LOCATION'], ['P', 'LOCATION']]\n",
      "[['N. M.', 'LOCATION'], ['L', 'PERSON'], ['Tu', 'LOCATION']]\n",
      "[['Pa.', 'LOCATION'], ['Angeles', 'LOCATION'], ['Utility Trailers', 'ORGANIZATION']]\n",
      "[['Bella Vista Place', 'LOCATION'], ['Gilbert', 'LOCATION'], ['Mesa', 'LOCATION']]\n",
      "[['ona', 'LOCATION'], ['Avery', 'PERSON'], ['Avery', 'ORGANIZATION']]\n",
      "[['Daly', 'PERSON'], ['Mesa', 'LOCATION'], ['Phoenix', 'LOCATION']]\n",
      "[['Central Avenue', 'LOCATION'], ['Ark', 'LOCATION'], ['.', 'PERSON']]\n",
      "[['Ba', 'PERSON'], ['lma', 'PERSON'], ['Associated Press', 'ORGANIZATION']]\n",
      "[['uren', 'LOCATION'], ['Plymouth Rocks', 'LOCATION'], ['Co', 'LOCATION']]\n",
      "[['Phoenix', 'LOCATION'], ['Phoenix', 'LOCATION'], ['Pasadena', 'LOCATION']]\n",
      "[['Scott', 'PERSON'], ['Pearl St.', 'LOCATION'], ['N. Y', 'LOCATION']]\n",
      "[['Hugo Richards', 'PERSON'], ['Oscar', 'PERSON'], ['IZONA R', 'LOCATION']]\n",
      "[['Taft', 'PERSON'], ['O. M.Payne', 'PERSON'], ['York', 'LOCATION']]\n",
      "[['Kansas City', 'LOCATION'], ['mpeN', 'LOCATION'], ['Fort Scott', 'LOCATION']]\n",
      "[['Fourth Street', 'LOCATION'], ['Brock & Feagans', 'ORGANIZATION'], ['Center St', 'LOCATION']]\n",
      "[['Giant Hit', 'ORGANIZATION'], ['O', 'LOCATION'], ['Boston', 'LOCATION']]\n",
      "[['Glacier Point', 'LOCATION'], ['Mountain House', 'LOCATION'], ['Overhanging Rock', 'LOCATION']]\n",
      "[['North Center Street', 'LOCATION'], ['Lu', 'ORGANIZATION'], ['Mac', 'PERSON']]\n",
      "[['Sacramento', 'LOCATION'], ['W', 'LOCATION'], ['Cal', 'LOCATION']]\n",
      "[['New Mexico', 'LOCATION'], ['California', 'LOCATION'], ['Nevada', 'LOCATION']]\n",
      "[['U', 'PERSON'], ['Mount R', 'LOCATION'], ['Dubuc', 'LOCATION']]\n",
      "[['G', 'PERSON'], ['The Associated Press', 'ORGANIZATION'], ['Albuquerque', 'LOCATION']]\n",
      "[['J. Brashears Jr', 'PERSON'], ['MA', 'LOCATION'], ['Washington,', 'LOCATION']]\n",
      "[['Or', 'LOCATION'], ['West Texas', 'LOCATION'], ['Southern California', 'LOCATION']]\n",
      "[['Wellington', 'LOCATION']]\n",
      "[['Phoenix', 'LOCATION'], ['E', 'PERSON'], ['Tucson', 'LOCATION']]\n",
      "[['IZONA', 'LOCATION'], ['W. E. Hatcher', 'PERSON'], ['A', 'LOCATION']]\n",
      "[['California', 'LOCATION'], ['Market Street', 'LOCATION'], ['R', 'ORGANIZATION']]\n",
      "[['ter County', 'LOCATION'], ['. M. Kleckley', 'PERSON'], ['Frank Clark', 'PERSON']]\n",
      "[['IZONA', 'LOCATION'], ['K', 'LOCATION']]\n",
      "[['A', 'LOCATION'], ['IZON', 'LOCATION'], ['Redond', 'LOCATION']]\n",
      "[['Phoenix', 'LOCATION'], ['ThomasWils', 'PERSON'], ['Morrison', 'PERSON']]\n",
      "[['Arizona', 'LOCATION'], ['Arizona', 'LOCATION'], ['MA', 'LOCATION']]\n",
      "[['A. Spalding', 'PERSON'], ['Ma', 'LOCATION'], ['Charley Hall', 'PERSON']]\n",
      "[['s. W.Clark', 'PERSON'], ['Cha', 'PERSON'], ['Clarkdale', 'LOCATION']]\n",
      "[['Perfecting Press', 'ORGANIZATION'], ['N', 'LOCATION'], ['Arizona', 'LOCATION']]\n",
      "[['rand Canyon', 'LOCATION'], ['Colorado', 'LOCATION'], ['CA', 'LOCATION']]\n",
      "[['Franklin', 'LOCATION'], ['IZONA', 'LOCATION'], ['Tucson', 'LOCATION']]\n",
      "[['Hero', 'LOCATION'], ['N', 'LOCATION'], ['land Stanford', 'PERSON']]\n",
      "[['E', 'LOCATION'], ['agua', 'LOCATION'], ['Madriz', 'LOCATION']]\n",
      "[['Jim Murphy', 'PERSON'], ['George King', 'PERSON'], ['. King', 'PERSON']]\n",
      "[['P', 'LOCATION'], ['Europe', 'LOCATION'], ['Cal.', 'LOCATION']]\n",
      "[['Layton', 'PERSON'], ['Phoenix', 'LOCATION'], ['Yavapai', 'LOCATION']]\n",
      "[['rizona', 'LOCATION'], ['IZONA', 'LOCATION'], ['IZONA', 'LOCATION']]\n",
      "[['San Xavier del Bac', 'LOCATION'], ['C', 'PERSON'], ['Mission', 'LOCATION']]\n",
      "[['tica', 'LOCATION'], ['Will E. Rapson', 'PERSON'], ['Russell', 'PERSON']]\n",
      "[['ogan', 'PERSON'], ['IZONA', 'LOCATION'], ['John', 'PERSON']]\n",
      "[['AR', 'LOCATION'], ['Moscow', 'LOCATION'], ['X', 'LOCATION']]\n",
      "[['M', 'LOCATION'], ['Phoenix', 'LOCATION'], ['City of', 'LOCATION']]\n",
      "[['M. K. Elder', 'PERSON'], ['America', 'LOCATION'], ['Louisville', 'LOCATION']]\n",
      "[['Porter', 'PERSON'], ['Arizona', 'LOCATION'], ['Arizona', 'LOCATION']]\n",
      "[['Adams Street', 'LOCATION'], ['Pi', 'ORGANIZATION'], ['Phoenix', 'LOCATION']]\n",
      "[['South Fourth Ave', 'LOCATION'], ['Wyoming', 'LOCATION'], ['IZONA', 'LOCATION']]\n",
      "[['Baltimore', 'LOCATION'], ['Louisville', 'LOCATION'], ['Denmark', 'LOCATION']]\n",
      "[['MA', 'LOCATION'], ['C', 'LOCATION'], ['Nally & Maitland', 'ORGANIZATION']]\n",
      "[['Louise Stevens Cooke', 'PERSON'], ['Phoenix', 'LOCATION'], ['MA', 'LOCATION']]\n",
      "[['East', 'LOCATION'], ['Steamship Co', 'ORGANIZATION'], ['London', 'LOCATION']]\n",
      "[['Ho', 'LOCATION'], ['Arizona', 'LOCATION'], ['outh Spring', 'LOCATION']]\n",
      "[['Bank of Commerce', 'ORGANIZATION'], ['NEW YORK C', 'LOCATION'], ['Lot', 'PERSON']]\n",
      "[['IZONA', 'LOCATION'], ['Mary', 'PERSON'], ['AR', 'LOCATION']]\n",
      "[['nik', 'LOCATION'], ['Lee', 'PERSON'], ['m', 'PERSON']]\n",
      "[['uena Vista', 'LOCATION'], ['JohnWix Thomas', 'PERSON'], ['B', 'LOCATION']]\n",
      "[['Casey', 'PERSON'], ['Duncan', 'PERSON'], ['IZONA', 'LOCATION']]\n",
      "[['P', 'LOCATION'], ['Los Angeles', 'LOCATION'], ['NE', 'LOCATION']]\n",
      "[['Monroe', 'LOCATION'], ['IZONA', 'LOCATION'], ['Monroe', 'LOCATION']]\n",
      "[['American', 'ORGANIZATION'], ['Baldwin Locomotive', 'ORGANIZATION'], ['American WritingP', 'ORGANIZATION']]\n",
      "[['MesaCity', 'LOCATION'], ['Pratt-', 'ORGANIZATION'], ['Gilbert', 'LOCATION']]\n",
      "[['Old Cumberland', 'LOCATION'], ['C', 'LOCATION'], ['hat', 'LOCATION']]\n",
      "[['James Monroe', 'PERSON'], ['Harvard', 'LOCATION'], ['Thomas Jefferson', 'PERSON']]\n",
      "[['Arizona', 'LOCATION'], ['F', 'LOCATION'], ['Phoenix', 'LOCATION']]\n",
      "[['South Third Avenue', 'LOCATION'], ['North First St', 'LOCATION'], ['North Central Avenue', 'LOCATION']]\n",
      "[['IZONA', 'LOCATION'], ['Eastlake Park', 'LOCATION']]\n",
      "[['IL', 'PERSON'], ['Phoenix', 'LOCATION'], ['Phoenix', 'LOCATION']]\n",
      "[['United States', 'LOCATION'], ['United States', 'LOCATION'], ['Genoa', 'LOCATION']]\n",
      "[['IZONA', 'LOCATION'], ['R', 'ORGANIZATION'], ['R', 'ORGANIZATION']]\n"
     ]
    }
   ],
   "source": [
    "# Grab 3 random labels from 100 newspapers:\n",
    "import random\n",
    "\n",
    "def loadJSONContent(jsonFilePath):\n",
    "    with open(jsonFilePath, \"r\") as dataFile:\n",
    "        return json.load(dataFile)\n",
    "    \n",
    "projectDataDict = loadJSONContent(projectNERJSON)\n",
    "\n",
    "randomNewspaperNERLabels = random.sample(sorted(projectDataDict.items()), 100)\n",
    "\n",
    "\n",
    "for newsTitle, labelList in randomNewspaperNERLabels:\n",
    "    if len(labelList) > 3:\n",
    "        threeRandomSelection = random.sample(labelList, 3)\n",
    "        print(threeRandomSelection)\n",
    "    else:\n",
    "        print(labelList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b37218",
   "metadata": {},
   "source": [
    "Looking at the print out, I can see that there is a good amount of accurate labels, such as `['Eddie Rickenbacker', 'PERSON']` and `['Phoenix', 'LOCATION']`. However, once in a while we would see inaccurate labels when a single letter is assigned with a label, such as `['E', 'PERSON']` and `['C', 'PERSON']`.\n",
    "\n",
    "The model is also able to accurately identify some rather advanced abbreviations, such as labeling N.M. (as in New Mexico) as such: `['N. M.', 'LOCATION']` or NY as such: `['N. Y', 'LOCATION']`. The model can also identify incomplete words, such as \"Calif\" as California, a location: `['Calif', 'LOCATION']`.\n",
    "\n",
    "This means that the model, although not perfect, performs at an acceptable accuracy and has a high `precision` score. This also means that, although the model was trained on more contemporary language, it can still identify the Named Entity usage in newspaper language mover 100 years ago."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs372fall25torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
