{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fW52vXux5sg"
      },
      "source": [
        "This file runs the fine-tuned BERT model on newspaper content collected from the Library of Congress and labels three NERs: Person, Location, and Organization. The program here then stores the labeled data in a JSON file.\n",
        "\n",
        "Code attribution: most of the code here was written or debugged by Claude Sonnett 4.5. See each individual code cell for detailed attribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_KEMDsNyVIo",
        "outputId": "24255dc3-592d-4449-ae05-2c5e6c94d7d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully from /Users/Jerry/Desktop/CS372/CoNLLFine-TunedBERT1BestPerforming\n",
            "Number of labels: 9\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned BERT model\n",
        "'''Code attribute: the entire code cell was written by Claude Sonnett 4.5 with edits and debugs made by me'''\n",
        "import torch, json\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "modelPath = \"/Users/Jerry/Desktop/CS372/CoNLLFine-TunedBERT1BestPerforming\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(modelPath)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelPath)\n",
        "\n",
        "print(f\"Model loaded successfully from {modelPath}\")\n",
        "print(f\"Number of labels: {model.config.num_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify file path to the JSON that stores the newspaper txt\n",
        "inputJsonFile = \"/Users/Jerry/Desktop/CS372/FinalProject/data/newspaperCleanedContent.json\"  \n",
        "\n",
        "# Also specify a JSON path that stores the labeled NER\n",
        "outputJsonFile = \"entitiesOutput.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9YMmzQPxzME_"
      },
      "outputs": [],
      "source": [
        "# Load label map\n",
        "'''Code attribution: from CoNLL 2003 dataset Hugging Face documentation: https://huggingface.co/datasets/eriktks/conll2003'''\n",
        "id2label = {\n",
        "    0: \"O\",\n",
        "    1: \"B-PER\",\n",
        "    2: \"I-PER\",\n",
        "    3: \"B-ORG\",\n",
        "    4: \"I-ORG\",\n",
        "    5: \"B-LOC\",\n",
        "    6: \"I-LOC\",\n",
        "    7: \"B-MISC\",\n",
        "    8: \"I-MISC\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDYgFMuYuzgj",
        "outputId": "e3bfb6f1-57d7-48ed-f626-05a7bea0692e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading /Users/Jerry/Desktop/CS372/FinalProject/data/newspaperCleanedContent.json...\n",
            "Processing 529 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting entities: 100%|██████████| 529/529 [02:42<00:00,  3.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving results to entitiesOutput.json...\n",
            "Done! Processed 529 documents.\n",
            "Total entities extracted: 8347\n",
            "\n",
            "Entity type breakdown:\n",
            "  PERSON: 2284\n",
            "  ORGANIZATION: 869\n",
            "  LOCATION: 5194\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# This code cell runs the fine-tuned BERT model on the 529 newspaper documents and stores the NER labels in a JSON file.\n",
        "'''Code attribute: most of the two functions ()\"extractEntities\" and \"processJsonFile\") are written by Claude Sonnet 4.5. I've made changes to variable data types and file paths as well as added the functionality to report on inference time'''\n",
        "\n",
        "def extractEntities(text, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Extract named entities from text using the fine-tuned BERT model.\n",
        "    Returns a list of [entity_text, entity_type] pairs.\n",
        "    \"\"\"\n",
        "    # Tokenize the text\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    offsetMapping = encoding.pop(\"offset_mapping\").squeeze().tolist()\n",
        "    inputIds = encoding[\"input_ids\"].to(device)\n",
        "    attentionMask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputIds, attention_mask=attentionMask)\n",
        "        predictions = outputs.logits.argmax(dim=-1).squeeze().tolist()\n",
        "\n",
        "    # Convert tokens back to words and extract entities\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputIds.squeeze().tolist())\n",
        "\n",
        "    entities = []\n",
        "    currentEntity = []\n",
        "    currentEntityType = None\n",
        "    currentEntityOffsets = []\n",
        "\n",
        "    for idx, (token, predId, (start, end)) in enumerate(zip(tokens, predictions, offsetMapping)):\n",
        "        # Skip special tokens and padding\n",
        "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"] or start == end == 0:\n",
        "            # Save any ongoing entity before skipping\n",
        "            if currentEntity:\n",
        "                entityText = text[currentEntityOffsets[0][0]:currentEntityOffsets[-1][1]]\n",
        "                entities.append([entityText, currentEntityType])\n",
        "                currentEntity = []\n",
        "                currentEntityType = None\n",
        "                currentEntityOffsets = []\n",
        "            continue\n",
        "\n",
        "        label = id2label.get(predId, \"O\")\n",
        "\n",
        "        if label.startswith(\"B-\"):  # Beginning of new entity\n",
        "            # Save previous entity if exists\n",
        "            if currentEntity:\n",
        "                entityText = text[currentEntityOffsets[0][0]:currentEntityOffsets[-1][1]]\n",
        "                entities.append([entityText, currentEntityType])\n",
        "\n",
        "            # Start new entity\n",
        "            entityType = label[2:]  # Remove \"B-\" prefix\n",
        "            if entityType in [\"PER\", \"ORG\", \"LOC\"]:  # Only keep the three types you want\n",
        "                currentEntity = [token]\n",
        "                currentEntityType = \"PERSON\" if entityType == \"PER\" else \\\n",
        "                                     \"ORGANIZATION\" if entityType == \"ORG\" else \"LOCATION\"\n",
        "                currentEntityOffsets = [(start, end)]\n",
        "            else:\n",
        "                currentEntity = []\n",
        "                currentEntityType = None\n",
        "                currentEntityOffsets = []\n",
        "\n",
        "        elif label.startswith(\"I-\") and currentEntity:  # Inside ongoing entity\n",
        "            entityType = label[2:]\n",
        "            expectedType = \"PER\" if currentEntityType == \"PERSON\" else \\\n",
        "                           \"ORG\" if currentEntityType == \"ORGANIZATION\" else \"LOC\"\n",
        "\n",
        "            if entityType == expectedType:  # Continue same entity\n",
        "                currentEntity.append(token)\n",
        "                currentEntityOffsets.append((start, end))\n",
        "            else:  # Type mismatch, save previous and reset\n",
        "                entityText = text[currentEntityOffsets[0][0]:currentEntityOffsets[-1][1]]\n",
        "                entities.append([entityText, currentEntityType])\n",
        "                currentEntity = []\n",
        "                currentEntityType = None\n",
        "                currentEntityOffsets = []\n",
        "\n",
        "        else:  # \"O\" or other label\n",
        "            if currentEntity:\n",
        "                entityText = text[currentEntityOffsets[0][0]:currentEntityOffsets[-1][1]]\n",
        "                entities.append([entityText, currentEntityType])\n",
        "                currentEntity = []\n",
        "                currentEntityType = None\n",
        "                currentEntityOffsets = []\n",
        "\n",
        "    # Don't forget the last entity\n",
        "    if currentEntity:\n",
        "        entityText = text[currentEntityOffsets[0][0]:currentEntityOffsets[-1][1]]\n",
        "        entities.append([entityText, currentEntityType])\n",
        "\n",
        "    return entities\n",
        "\n",
        "\n",
        "def processJsonFile(inputFile, outputFile, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Process the entire JSON file and extract entities for all texts.\n",
        "    \"\"\"\n",
        "    # Load input JSON\n",
        "    print(f\"Loading {inputFile}...\")\n",
        "    with open(inputFile, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Process each text\n",
        "    results = {}\n",
        "    print(f\"Processing {len(data)} documents...\")\n",
        "\n",
        "    totalInferenceTime = 0\n",
        "    totalDocumentsProcessed = 0\n",
        "    totalWordCount = 0\n",
        "    for title, text in tqdm(data.items(), desc=\"Extracting entities\"):\n",
        "        startTime = time.time()\n",
        "        entities = extractEntities(text, model, tokenizer, device)\n",
        "        endTime = time.time()\n",
        "        totalWordCount += len(text.split())\n",
        "        inferenceTime = endTime - startTime\n",
        "        totalInferenceTime += inferenceTime\n",
        "        totalDocumentsProcessed += 1\n",
        "        results[title] = entities\n",
        "\n",
        "    # Save results\n",
        "    print(f\"Saving results to {outputFile}...\")\n",
        "    with open(outputFile, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Done! Processed {len(results)} documents.\")\n",
        "    print(f\"Total entities extracted: {sum(len(v) for v in results.values())}\")\n",
        "\n",
        "    # Print some statistics\n",
        "    entityCounts = {\"PERSON\": 0, \"ORGANIZATION\": 0, \"LOCATION\": 0}\n",
        "    for entities in results.values():\n",
        "        for entity, entityType in entities:\n",
        "            entityCounts[entityType] = entityCounts.get(entityType, 0) + 1\n",
        "\n",
        "    print(\"\\nEntity type breakdown:\")\n",
        "    for entityType, count in entityCounts.items():\n",
        "        print(f\"  {entityType}: {count}\")\n",
        "\n",
        "    return totalInferenceTime, totalDocumentsProcessed, totalWordCount\n",
        "\n",
        "\n",
        "# Run the processing\n",
        "\n",
        "totalInferenceTime, totalDocumentsProcessed, totalWordCount = processJsonFile(inputJsonFile, outputJsonFile, model, tokenizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IiEtLRV023M",
        "outputId": "9ea5fdbb-b3b0-4a8c-a71f-9518feb9ef82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average inference time to process one newspaper: 0.023015342723219066 when average word count per document is: 2923.742911153119\n"
          ]
        }
      ],
      "source": [
        "# Find inference time:\n",
        "'''Code attribute: all code mine'''\n",
        "\n",
        "averageTimeToProcessOneNewspaper = totalInferenceTime / totalDocumentsProcessed\n",
        "averageWordCountPerDocument = totalWordCount / totalDocumentsProcessed\n",
        "print(f\"Average inference time to process one newspaper: {averageTimeToProcessOneNewspaper} when average word count per document is: {averageWordCountPerDocument}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpMm8smr2eob"
      },
      "source": [
        "I'm calculating the inference time per document through averaging all inference times when iterating through all documents in my dataset. To put the average inference time in perspective, I've also calculated the average word length of each document.\n",
        "\n",
        "Average inference time: 0.023 sec.\n",
        "Average word count per document: 2923.\n",
        "\n",
        "Hardware specs: Running model using A100 GPU with 80GB GPU RAM. Running model through Google Colab Pro for Education account."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs372fall25torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
