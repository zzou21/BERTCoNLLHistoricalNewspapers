{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7219c52",
   "metadata": {},
   "source": [
    "This notebook showcases the code necessary to connect with Library of Congress's <i>Chronicling America</i> database API to extract specific subsets of newspaper metadata and full OCR-ed texts.\n",
    "\n",
    "Code Attribution: Most of the code was provided by <i>Chronicling America</i> (see API documentation page: https://libraryofcongress.github.io/data-exploration/loc.gov%20JSON%20API/Chronicling_America/ChronAm_analyzing_specific_titles_limit_results.html). See detailed attributions in specific code cells below. After modifying the code, I used the Duke Compute Cluster (see slurm script in corresponding code cell below) to obtain enough computing power to complete the data processing steps as well as learning how to use a hih-performance computer cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The majority of this code cell was provided by the Chronicling America API, except for places that I've specified'''\n",
    "import requests, pandas as pd\n",
    "\n",
    "# Accessing search URL. Edit this URL to reflect the search criteria needed.\n",
    "searchURLTest = \"https://www.loc.gov/newspapers/?end_date=1930-01-01&ops=~10&qs=los+angeles+oil&searchType=advanced&start_date=1890-01-01&location_country=united+states&fo=json\"\n",
    "\n",
    "# I added this counter to keep track of how many search results we have.\n",
    "numberOfResults = 0 \n",
    "\n",
    "def get_item_ids_test(url, items=[], conditional='True'):\n",
    "    global numberOfResults\n",
    "    # Check that the query URL is not an item or resource link.\n",
    "    exclude = [\"loc.gov/item\",\"loc.gov/resource\"]\n",
    "    if any(string in url for string in exclude):\n",
    "        raise NameError('Your URL points directly to an item or '\n",
    "                        'resource page (you can tell because \"item\" '\n",
    "                        'or \"resource\" is in the URL). Please use '\n",
    "                        'a search URL instead. For example, instead '\n",
    "                        'of \\\"https://www.loc.gov/item/2009581123/\\\", '\n",
    "                        'try \\\"https://www.loc.gov/maps/?q=2009581123\\\". ')\n",
    "\n",
    "    # request pages of 100 results at a time\n",
    "    params = {\"fo\": \"json\", \"c\": 100, \"at\": \"results,pagination\"}\n",
    "    call = requests.get(url, params=params)\n",
    "    # Check that the API request was successful\n",
    "    if (call.status_code==200) & ('json' in call.headers.get('content-type')):\n",
    "        data = call.json()\n",
    "        results = data['results'] # deleted the top 20 limit\n",
    "        for result in results:\n",
    "            # Filter out anything that's a collection or web page\n",
    "            filter_out = (\"collection\" in result.get(\"original_format\")) \\\n",
    "                    or (\"web page\" in result.get(\"original_format\")) \\\n",
    "                    or (eval(conditional)==False)\n",
    "            if not filter_out:\n",
    "                # Get the link to the item record\n",
    "                if result.get(\"id\"):\n",
    "                    item = result.get(\"id\")\n",
    "                    # Filter out links to Catalog or other platforms\n",
    "                    if item.startswith(\"http://www.loc.gov/resource\"):\n",
    "                      resource = item  # Assign item to resource\n",
    "                      items.append(resource)\n",
    "                    if item.startswith(\"http://www.loc.gov/item\"):\n",
    "                      items.append(item)\n",
    "            \n",
    "            # Added the following two lines to keep track of the number of results processed\n",
    "            numberOfResults += 1\n",
    "            print(f\"Processed {numberOfResults} results.\")\n",
    "            \n",
    "        # Repeat the loop on the next page, unless we're on the last page.\n",
    "        if data[\"pagination\"][\"next\"] is not None:\n",
    "            next_url = data[\"pagination\"][\"next\"]\n",
    "            print(f\"Total number of pages: {data['pagination']['total']}\")\n",
    "            get_item_ids_test(next_url, items, conditional)\n",
    "\n",
    "        return items\n",
    "    else:\n",
    "            print('There was a problem. Try running the cell again, or check your searchURL.')\n",
    "\n",
    "# Generate a list of records found from performing a query and save these Item IDs. (Create ids_list based on items found in the searchURL result)\n",
    "ids_list_test = get_item_ids_test(searchURLTest, items=[])\n",
    "\n",
    "# Add 'fo=json' to the end of each row in ids_list (All individual ids from from the ids_list are now listed in JSON format in new_ids)\n",
    "ids_list_json_test = []\n",
    "for id in ids_list_test:\n",
    "  if not id.endswith('&fo=json'):\n",
    "    id += '&fo=json'\n",
    "  ids_list_json_test.append(id)\n",
    "ids = ids_list_json_test\n",
    "\n",
    "print('\\nSuccess! Your API Search Query found '+str(len(ids_list_json_test))+' related newspaper pages. Proceed to the next step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The majority of this code cell was provided by the Chronicling America API, except for places that I've specified'''\n",
    "\n",
    "# Create a list of dictionaries to store the item metadata\n",
    "item_metadata_list = []\n",
    "\n",
    "# I created this counter to keep track of the number of newspapers processed\n",
    "counter = 0\n",
    "\n",
    "# Iterate over the list of item IDs\n",
    "for item_id in ids_list_json_test:\n",
    "  item_response = requests.get(item_id)\n",
    "\n",
    "  # Check if the API call was successful and Parse the JSON response\n",
    "  if item_response.status_code == 200:\n",
    "    # Iterate over the ids_list_json list and extract the relevant metadata from each dictionary.\n",
    "    item_data = item_response.json()\n",
    "    # NOT filtering out newspapers that do not have a city associated with it.\n",
    "    # if 'location_city' not in item_data['item']:\n",
    "    #   continue\n",
    "\n",
    "    # Extract the relevant item metadata\n",
    "    Newspaper_Title = item_data['item']['newspaper_title']\n",
    "    Issue_Date = item_data['item']['date']\n",
    "    Page = item_data['pagination']['current']\n",
    "    State = item_data['item']['location_state']\n",
    "    City = item_data['item']['location_city']\n",
    "    LCCN = item_data['item']['number_lccn']\n",
    "    Contributor = item_data['item']['contributor_names']\n",
    "    Batch = item_data['item']['batch']\n",
    "    pdf = item_data['resource']['pdf']\n",
    "\n",
    "    # Add the item metadata to the list\n",
    "    item_metadata_list.append({\n",
    "        'Newspaper Title': Newspaper_Title,\n",
    "        'Issue Date': Issue_Date,\n",
    "        'Page Number': Page,\n",
    "        'LCCN': LCCN,\n",
    "        'City': City,\n",
    "        'State': State,\n",
    "        'Contributor': Contributor,\n",
    "        'Batch': Batch,\n",
    "        'PDF Link': pdf,\n",
    "    })\n",
    "\n",
    "    # I added the following two lines to keep track of the number of results processed.\n",
    "    counter += 1\n",
    "    print(f\"Processed {counter} results.\")\n",
    "    \n",
    "\n",
    "# Change date format to MM-DD-YYYY\n",
    "for item in item_metadata_list:\n",
    "  item['Issue Date'] = pd.to_datetime(item['Issue Date']).strftime('%m-%d-%Y')\n",
    "\n",
    "# Create a Pandas DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(item_metadata_list)\n",
    "\n",
    "print('\\nSuccess! Ready to proceed to the next step!')\n",
    "\n",
    "# Add your Local saveTo Location (e.g. C:/Downloads/)\n",
    "saveTo = '/hpc/home/zz341/test4'\n",
    "\n",
    "# Set File Name. Make sure to rename the file so it doesn't overwrite previous!\n",
    "filename = 'LOCLAOilInitialExtract'\n",
    "\n",
    "metadata_dataframe = pd.DataFrame(item_metadata_list)\n",
    "metadata_dataframe.to_csv(saveTo + '/' + filename + '.csv')\n",
    "print(\"Finished compiling CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1aff9e",
   "metadata": {},
   "source": [
    "This was the code I wrote to use the Duke Compute Cluster to access the API. I used DCC because of the large amount of newspaper search results that my computer's memory couldn't hold all the search results. I wrote this slurm script to access and use DCC:\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH --output=outputNov21.out\n",
    "#SBATCH --error=errorNov21.err\n",
    "#SBATCH --mem=7G\n",
    "#SBATCH --partition=common\n",
    "\n",
    "srun --cpu-bind=none python /hpc/home/zz341/test4/testLOCExtract.py\n",
    "```\n",
    "\n",
    "I then saved the data in a CSV. This API call returned about 15,100 results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae6948",
   "metadata": {},
   "source": [
    "Because Library of Congress stores their OCR-ed texts as labeled XML files, I needed to download ~15,100 XMLs to my computer and parse them into TXT files. I used the following code to do so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This was the Python object I wrote to turn Library of Congress newspaper OCR text XMLs into TXTs. This resulted in about 15,100 TXT files (one for each search result). Claude Sonnet 3.5 coded most of the code. I designed this part into a Python object so that it could be reusable for other people and other purposes.'''\n",
    "\n",
    "import os, csv, requests, re, gc, io\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class ExtractCSV:\n",
    "    def __init__(self, CSVFilePath: str):\n",
    "        self.CSVFilePath = CSVFilePath\n",
    "        self.csvDataFrame = pd.read_csv(CSVFilePath)\n",
    "\n",
    "    def makeFileName(self, partOfFileName):\n",
    "        \"\"\"Make string safe for filenames (remove spaces and bad chars).\"\"\"\n",
    "        return re.sub(r'[^A-Za-z0-9_-]+', '', partOfFileName.replace(\" \", \"_\"))\n",
    "\n",
    "    def xmlToPlainText(self, xmlContent):\n",
    "        \"\"\"Convert ALTO XML content into plain text with newlines preserved (namespace-agnostic).\"\"\"\n",
    "        # Remove namespaces by reparsing\n",
    "        it = ET.iterparse(io.BytesIO(xmlContent))\n",
    "        for _, el in it:\n",
    "            if \"}\" in el.tag:\n",
    "                el.tag = el.tag.split(\"}\", 1)[1]  # strip namespace\n",
    "        root = it.root\n",
    "\n",
    "        all_lines = []\n",
    "        for textblock in root.findall(\".//TextBlock\"):\n",
    "            block_lines = []\n",
    "            for textline in textblock.findall(\"TextLine\"):\n",
    "                words = [\n",
    "                    string.attrib.get(\"CONTENT\", \"\")\n",
    "                    for string in textline.findall(\"String\")\n",
    "                ]\n",
    "                if words:\n",
    "                    block_lines.append(\" \".join(words))\n",
    "            if block_lines:\n",
    "                all_lines.append(\"\\n\".join(block_lines))\n",
    "\n",
    "        return \"\\n\\n\".join(all_lines)\n",
    "\n",
    "\n",
    "    def turnColumnIntoList(self, column=\"PDF Link\"):\n",
    "        \"\"\"Convert a column into a Python list.\"\"\"\n",
    "        return self.csvDataFrame[column].tolist()\n",
    "\n",
    "    def turnPDFLinkIntoXMLLink(self):\n",
    "        \"\"\"Return list of (PDFLink, XMLLink).\"\"\"\n",
    "        pdf_links = self.turnColumnIntoList(\"PDF Link\")\n",
    "        return [(link, link[:-3] + \"xml\") for link in pdf_links]\n",
    "\n",
    "    def downloadAndProcessXML(self, txtDirectory=\"txt_out\", xmlDirectory=\"xml_out\", failedFile=\"failed_links.txt\"):\n",
    "        \"\"\"Download XML, save raw XML and parsed TXT with descriptive names. \n",
    "        Log any failed/empty results into a separate file.\"\"\"\n",
    "        os.makedirs(txtDirectory, exist_ok=True)\n",
    "        os.makedirs(xmlDirectory, exist_ok=True)\n",
    "\n",
    "        failedLinks = []\n",
    "\n",
    "        for i, row in self.csvDataFrame.iterrows():\n",
    "            try:\n",
    "                # Build filename base from metadata\n",
    "                filename_base = \"_\".join([\n",
    "                    self.makeFileName(str(row[\"NewspaperTitle\"])),\n",
    "                    self.makeFileName(str(row[\"IssueDate\"])),\n",
    "                    self.makeFileName(str(row[\"City\"])),\n",
    "                    self.makeFileName(str(row[\"State\"])),\n",
    "                    self.makeFileName(str(row[\"Region\"])),\n",
    "                    f\"p{row['PageNumber']}\",\n",
    "                ])\n",
    "\n",
    "                xmlURLLink = row[\"PDF Link\"][:-3] + \"xml\"\n",
    "\n",
    "                # Download XML\n",
    "                response = requests.get(xmlURLLink, stream=True)\n",
    "                response.raise_for_status()\n",
    "                xmlContent = response.content\n",
    "\n",
    "                # Save raw XML\n",
    "                xmlPath = os.path.join(xmlDirectory, filename_base + \".xml\")\n",
    "                with open(xmlPath, \"wb\") as f:\n",
    "                    f.write(xmlContent)\n",
    "\n",
    "                # Parse XML into TXT\n",
    "                text = self.xmlToPlainText(xmlContent)\n",
    "                txtPath = os.path.join(txtDirectory, filename_base + \".txt\")\n",
    "                with open(txtPath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(text)\n",
    "\n",
    "                if not text.strip():\n",
    "                    failedLinks.append(xmlURLLink)\n",
    "                    print(f\"[{i+1}/{len(self.csvDataFrame)}] EMPTY TEXT for {xmlURLLink}\")\n",
    "                else:\n",
    "                    print(f\"[{i+1}/{len(self.csvDataFrame)}] Saved {txtPath} and {xmlPath}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with row {i} ({row['PDF Link']}): {e}\")\n",
    "                failedLinks.append(row[\"PDF Link\"])\n",
    "\n",
    "            finally:\n",
    "                # Free memory after each file\n",
    "                del response, xmlContent\n",
    "                gc.collect()\n",
    "\n",
    "        # Save failed links to file\n",
    "        if failedLinks:\n",
    "            with open(failedFile, \"w\", encoding=\"utf-8\") as f:\n",
    "                for link in failedLinks:\n",
    "                    f.write(link + \"\\n\")\n",
    "            print(f\"\\nSaved {len(failedLinks)} failed links to {failedFile}\")\n",
    "        else:\n",
    "            print(\"\\nAll files processed with non-empty text.\")\n",
    "\n",
    "\n",
    "    def checkBlankCells(self, column=\"Region\"):\n",
    "        \"\"\"Check how many blank cells are in a given column.\"\"\"\n",
    "        blankCounter = 0\n",
    "        with open(self.CSVFilePath, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "\n",
    "            if column not in reader.fieldnames:\n",
    "                raise ValueError(f\"CSV does not contain a '{column}' column.\")\n",
    "\n",
    "            for row in reader:\n",
    "                if row[column] is None or row[column].strip() == \"\":\n",
    "                    blankCounter += 1\n",
    "\n",
    "        return blankCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d7c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Run the XML to TXT python object'''\n",
    "\n",
    "# This CSV stores the newspaper metadata from the API call.\n",
    "CSVFilePath = \"/Users/Jerry/Desktop/DHproj-reading/LAOilProject/LAOil/DataFolder/testCSVNewspaperWithXMLLinks.csv\"\n",
    "extractor = ExtractCSV(CSVFilePath)\n",
    "\n",
    "# Choose your own folders\n",
    "txtFolder = \"/Volumes/JZ/LAOilTXTXML/TXTOutput\"\n",
    "xmlFolder = \"/Volumes/JZ/LAOilTXTXML/XMLOutput\"\n",
    "\n",
    "extractor.downloadAndProcessXML(txtDirectory=txtFolder, xmlDirectory=xmlFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cb0fa",
   "metadata": {},
   "source": [
    "After storing all TXT files in the TXT folder, we will use the `LLMAPICleaning.py` file to clean our messy OCR texts (with spelling or scanning errors) into cleaned and edited OCRs suitable for analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs372fall25torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
